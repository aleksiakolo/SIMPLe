model:
  tokenizer: "facebook/bart-large"
  decoder: "facebook/bart-large"
  embedding_dim: 768
  num_heads: 8
  ff_dim: 2048
  num_layers: 4
  dropout: 0.1
  max_output_length: 512

training:
  batch_size: 8
  learning_rate: 3e-5
  max_epochs: 10
  gpus: 1

data:
  train:
    data_dir: "/home/aleksia/lex_sum_data/data/test"
    labels_dir: "/home/aleksia/lex_sum_data/labels/test"
  val:
    data_dir: "/home/aleksia/lex_sum_data/data/validation"
    labels_dir: "/home/aleksia/lex_sum_data/labels/validation"

callbacks:
  checkpoint:
    dirpath: "./checkpoints"
  early_stopping:
    patience: 5

project_name: "LegalBERT-Summarization"
